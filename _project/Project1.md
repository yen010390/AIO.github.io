---
title: "Module 1: á»¨ng dá»¥ng RAG trong viá»‡c há»i Ä‘Ã¡p tÃ i liá»‡u bÃ i há»c AIO"
excerpt: "Dá»± Ã¡n nÃ y xÃ¢y dá»±ng há»‡ thá»‘ng há»i Ä‘Ã¡p thÃ´ng minh dÃ¹ng kiáº¿n trÃºc RAG, giÃºp ngÆ°á»i há»c khÃ³a AI táº¡i AI Viá»‡t Nam (AIO) khai thÃ¡c hiá»‡u quáº£ ná»™i dung tÃ i liá»‡u há»c táº­p."
collection: project
author: "Nguyá»…n Tuáº¥n Anh - ÄoÃ n Táº¥n HÆ°ng - Há»“ Thá»‹ Ngá»c Huyá»n - Tráº§n Thá»‹ Má»¹ TÃº - Äáº·ng Thá»‹ HoÃ ng Yáº¿n"
tags:
- RAG
- LLM
- Chatbot
---

**TÃ¡c giáº£**: Nguyá»…n Tuáº¥n Anh - ÄoÃ n Táº¥n HÆ°ng - Há»“ Thá»‹ Ngá»c Huyá»n - Tráº§n Thá»‹ Má»¹ TÃº - Äáº·ng Thá»‹ HoÃ ng Yáº¿n

<details>
<summary><strong>ğŸ“ Cáº¥u trÃºc source code  (click Ä‘á»ƒ xem)</strong></summary>

- Source code Ä‘Æ°á»£c Ä‘áº·t táº¡y Ä‘Ã¢y: https://github.com/aio25-mix002/m01-p0102
- Jupiter Notebooks: https://github.com/aio25-mix002/m01-p0102/blob/main/runbook_m01p0102.ipynb

<br/>

```

ğŸ“¦ RAG_AIO_Chatbot
â”œâ”€â”€ assets/                   # TÃ i sáº£n tÄ©nh (logo, favicon...)
â”‚   â””â”€â”€ logo.png              # Logo cá»§a á»©ng dá»¥ng
â”œâ”€â”€ examples/                 # Dá»¯ liá»‡u máº«u Ä‘á»ƒ test
â”‚   â””â”€â”€ YOLOv10_Tutorials.pdf # File PDF máº«u
â”œâ”€â”€ logs/                     # ThÆ° má»¥c lÆ°u log
â”œâ”€â”€ prompt_templates/         # CÃ¡c template prompt cho RAG
â”œâ”€â”€ utils/                    # CÃ¡c tiá»‡n Ã­ch há»— trá»£
â”‚   â”œâ”€â”€ logging_utils.py      # Utility logging
â”‚   â””â”€â”€ prompt_utils.py       # Utility quáº£n lÃ½ prompt
â”œâ”€â”€ .vscode/                  # Cáº¥u hÃ¬nh Visual Studio Code
â”‚   â””â”€â”€ launch.json           # Debug configuration
â”œâ”€â”€ .env                      # Biáº¿n mÃ´i trÆ°á»ng production
â”œâ”€â”€ .env.example              # Template biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ .env.local                # Biáº¿n mÃ´i trÆ°á»ng local
â”œâ”€â”€ rag_chatbot.py            # File chÃ­nh - Streamlit RAG chatbot
â”œâ”€â”€ runbook_m01p0102.ipynb    # Jupyter notebook hÆ°á»›ng dáº«n
â”œâ”€â”€ requirements.txt          # Dependencies chÃ­nh
â”œâ”€â”€ requirements-torch.txt    # Dependencies PyTorch
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ README.md                 # TÃ i liá»‡u hÆ°á»›ng dáº«n
```
</details>

<details>
<summary><strong>ğŸ“ Má»¥c lá»¥c bÃ¡o cÃ¡o (click Ä‘á»ƒ xem)</strong></summary>
<br/>

- [TÃ³m táº¯t](#tÃ³m-táº¯t)
- [1. Giá»›i thiá»‡u ğŸ—‚](#1-giá»›i-thiá»‡u-)
- [2. PhÆ°Æ¡ng phÃ¡p luáº­n ğŸ“š](#2-phÆ°Æ¡ng-phÃ¡p-luáº­n-)
  - [2.1. Quy trÃ¬nh Láº­p chá»‰ má»¥c dá»¯ liá»‡u (Indexing)](#21-quy-trÃ¬nh-láº­p-chá»‰-má»¥c-dá»¯-liá»‡u-indexing)
  - [2.2. Quy trÃ¬nh Truy váº¥n vÃ  Táº¡o sinh (Retrieval \& Generation)](#22-quy-trÃ¬nh-truy-váº¥n-vÃ -táº¡o-sinh-retrieval--generation)
- [3. Thá»±c hiá»‡n âš™](#3-thá»±c-hiá»‡n-)
- [4. Káº¿t quáº£ ğŸ“ˆ](#4-káº¿t-quáº£-)
- [5. Má»Ÿ rá»™ng nÃ¢ng cao ğŸ–¥](#5-má»Ÿ-rá»™ng-nÃ¢ng-cao-)
  - [5.1 TiÃªu chÃ­ cáº£i tiáº¿n](#51-tiÃªu-chÃ­-cáº£i-tiáº¿n)
  - [5.2 Code nÃ¢ng cao](#52-code-nÃ¢ng-cao)
    - [5.2.1 NÃ¢ng cáº¥p cá»‘t lá»—i: Ghi nhá»› lá»‹ch sá»­ há»™i thoáº¡i (Conversation memory)](#521-nÃ¢ng-cáº¥p-cá»‘t-lá»—i-ghi-nhá»›-lá»‹ch-sá»­-há»™i-thoáº¡i-conversation-memory)
    - [5.2.2 Quáº£n lÃ½ Vector DB nÃ¢ng cao](#522-quáº£n-lÃ½-vector-db-nÃ¢ng-cao)
    - [5.2.3. Gá»¡ lá»—i (Debugging) vá»›i Logger](#523-gá»¡-lá»—i-debugging-vá»›i-logger)
    - [5.2.4. Xá»­ lÃ½ vÃ  truy váº¥n tá»« nhiá»u file tÃ i liá»‡u](#524-xá»­-lÃ½-vÃ -truy-váº¥n-tá»«-nhiá»u-file-tÃ i-liá»‡u)
  - [5.3 Káº¿t quáº£ má»Ÿ rá»™ng ğŸ“](#53-káº¿t-quáº£-má»Ÿ-rá»™ng-)
    - [5.3.1 Há»— trá»£ ghi nhá»›](#531-há»—-trá»£-ghi-nhá»›)
    - [5.3.2 Xá»­ dá»¥ng táº­p tÃ i liá»‡u khÃ¡c á»©ng dá»¥ng trong y khoa](#532-xá»­-dá»¥ng-táº­p-tÃ i-liá»‡u-khÃ¡c-á»©ng-dá»¥ng-trong-y-khoa)
    - [5.3.3 Há»— trá»£ lÃ m viá»‡c vá»›i nhiá»u tÃ i liá»‡u khÃ¡c nhau](#533-há»—-trá»£-lÃ m-viá»‡c-vá»›i-nhiá»u-tÃ i-liá»‡u-khÃ¡c-nhau)
- [6. Káº¿t luáº­n ğŸ“Œ](#6-káº¿t-luáº­n-)

</details>

<br/>

# TÃ³m táº¯t
Máº·c dÃ¹ LLMs ráº¥t máº¡nh, chÃºng váº«n bá»‹ háº¡n cháº¿ vá» kiáº¿n thá»©c chuyÃªn ngÃ nh vÃ  tÃ­nh cáº­p nháº­t. Dá»± Ã¡n nÃ y xÃ¢y dá»±ng há»‡ thá»‘ng há»i Ä‘Ã¡p thÃ´ng minh dÃ¹ng kiáº¿n trÃºc RAG, giÃºp ngÆ°á»i há»c khÃ³a AI táº¡i AI Viá»‡t Nam (AIO) khai thÃ¡c hiá»‡u quáº£ ná»™i dung tÃ i liá»‡u há»c táº­p.


# 1. Giá»›i thiá»‡u ğŸ—‚ 
- CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs) nhÆ° ChatGPT cÃ³ kháº£ nÄƒng tráº£ lá»i linh hoáº¡t nhÆ°ng bá»‹ giá»›i háº¡n bá»Ÿi dá»¯ liá»‡u huáº¥n luyá»‡n, nÃªn khÃ´ng xá»­ lÃ½ tá»‘t thÃ´ng tin má»›i hoáº·c cÃ¡ nhÃ¢n hÃ³a.
- Äá»ƒ kháº¯c phá»¥c, kiáº¿n trÃºc Retrieval-Augmented Generation (RAG) cho phÃ©p LLM truy xuáº¥t thÃ´ng tin tá»« nguá»“n ngoÃ i (nhÆ° PDF, cÆ¡ sá»Ÿ dá»¯ liá»‡u) trÆ°á»›c khi táº¡o cÃ¢u tráº£ lá»i, giÃºp káº¿t quáº£ chÃ­nh xÃ¡c vÃ  phÃ¹ há»£p hÆ¡n.
- Má»¥c tiÃªu dá»± Ã¡n lÃ  xÃ¢y dá»±ng chatbot á»©ng dá»¥ng RAG, há»— trá»£ há»c viÃªn khÃ³a AIO há»i â€“ Ä‘Ã¡p trá»±c tiáº¿p dá»±a trÃªn ná»™i dung tÃ i liá»‡u bÃ i giáº£ng.

# 2. PhÆ°Æ¡ng phÃ¡p luáº­n ğŸ“š 
Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc RAG tiÃªu chuáº©n, bao gá»“m hai quy trÃ¬nh chÃ­nh: 
- Láº­p chá»‰ má»¥c dá»¯ liá»‡u (Indexing) 
- Truy váº¥n & Táº¡o sinh (Retrieval & Generation).


![Quy trÃ¬nh RAG tá»•ng quan](/AIO.github.io/images/M01/M01_RAG_1.png)

HÃ¬nh 1: SÆ¡ Ä‘á»“ tá»•ng quan vá» chÆ°Æ¡ng trÃ¬nh RAG trong project.


## 2.1. Quy trÃ¬nh Láº­p chá»‰ má»¥c dá»¯ liá»‡u (Indexing)

<details>
<summary> BÆ°á»›c 1: Táº£i dá»¯ liá»‡u â€“ Äá»c vÃ  trÃ­ch xuáº¥t vÄƒn báº£n tá»« file PDF (PyPDFLoader) </summary>

```python
# HÃ m PyPDFLoader
from langchain.document_loaders import PyPDFLoader

# Táº£i file PDF vÃ  trÃ­ch xuáº¥t vÄƒn báº£n
loader = PyPDFLoader(tmp_file_path)
documents = loader.load()
```
</details>

<details>
<summary>BÆ°á»›c 2: PhÃ¢n Ä‘oáº¡n â€“ Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunks) </summary>
Giáº£i phÃ¡p hiá»‡n táº¡i lÃ  sá»­ dá»¥ng SemanticChunker Ä‘á»ƒ chia cÃ¡c Ä‘oáº¡n dá»±a theo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá» máº·t ngá»¯ nghÄ©a (semantic similarity). 


QuÃ¡ trÃ¬nh nÃ y bao gá»“m viá»‡c tÃ¡ch vÄƒn báº£n thÃ nh tá»«ng cÃ¢u, sau Ä‘Ã³ nhÃ³m má»—i 3 cÃ¢u láº¡i vá»›i nhau, rá»“i há»£p nháº¥t cÃ¡c nhÃ³m cÃ³ ná»™i dung tÆ°Æ¡ng tá»± nhau dá»±a trÃªn khÃ´ng gian embedding.

```python
#HÃ m SemanticChunker
from langchain.text_splitter import SemanticChunker

semantic_splitter = SemanticChunker(
    embeddings = st.session_state.embeddings,
    buffer_size = 1,
    breakpoint_threshold_type = "percentile",
    breakpoint_threshold_amount = 95,
    min_chunk_size = 500,
    add_start_index = True
)
```

</details>

![Semantic Chunking](/AIO.github.io/images/M01/M01_RAG_3.png)

HÃ¬nh 2: SÆ¡ Ä‘á»“ vá» Semantic Chunking.

<details>
<summary>BÆ°á»›c 3: MÃ£ hÃ³a â€“ Chuyá»ƒn má»—i Ä‘oáº¡n vÄƒn báº£n thÃ nh vector sá»‘ há»c </summary>
CÃ¡c Ä‘oáº¡n vÄƒn báº£n dáº¡ng chuá»—i cáº§n Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i vá» dáº¡ng sá»‘ há»c Ä‘á»ƒ Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n xá»­ lÃ½ phÃ¹ há»£p. Viá»‡c nÃ y gá»i lÃ  encoding. 

Trong giáº£i phÃ¡p hiá»‡n táº¡i ta sá»­ dá»¥ng `bkai-foundation-models/vietnamese-bi-encoder` lÃ m mÃ´ hÃ¬nh embedding Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c Ä‘oáº¡n vÄƒn báº£n dáº¡ng chuá»—i sang khÃ´ng gian vector sá»‘.

Viá»‡c sá»­ dá»¥ng cáº¥u trÃºc dá»¯ liá»‡u vector giÃºp viá»‡c xá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n truy váº¥n vector Ä‘á»ƒ tÃ¬m kiáº¿m cÃ¡c vÄƒn báº£n tÆ°Æ¡ng Æ°á»›ng (vÃ­ dá»¥ thuáº­t toÃ¡n HNSW trong chroma database)


```python
# Sá»­ dá»¥ng mÃ´ hÃ¬nh embedding bkai-foundation-models/vietnamese-bi-encoder
from langchain.embeddings import HuggingFaceEmbeddings

def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")
```
</details>

![Vector database](/AIO.github.io/images/M01/M01_RAG_2.png)

HÃ¬nh 3: SÆ¡ Ä‘á»“ bÆ°á»›c thá»±c hiá»‡n xÃ¢y dá»±ng vector database.


<details>
<summary>BÆ°á»›c 4: LÆ°u trá»¯ â€“ LÆ°u cÃ¡c vector vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ truy váº¥n nhanh</summary>

Trong giáº£i phÃ¡p hiá»‡n táº¡i sá»­ dá»¥ng Chroma lÃ m vector database. 

```python    
from langchain.vectorstores import Chroma

#ChromaDB, langchain.vectorstores

# PhÃ¢n Ä‘oáº¡n vÃ  lÆ°u trá»¯ vector
docs = semantic_splitter.split_documents(documents)
vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
retriever = vector_db.as_retriever()

```

</details>


## 2.2. Quy trÃ¬nh Truy váº¥n vÃ  Táº¡o sinh (Retrieval & Generation)

<details>
<summary>BÆ°á»›c 1: MÃ£ hÃ³a cÃ¢u há»i â€“ Chuyá»ƒn cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng thÃ nh vector </summary>

TÆ°Æ¡ng tá»± quy trÃ¬nh láº­p chá»‰ má»¥c dá»¯ liá»‡u á»Ÿ trÃªn. Ta cáº§n chuyá»ƒn cÃ¢u há»i vá» khÃ´ng gian vector sá»‘ Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n truy váº¥n dá»¯ liá»‡u Ä‘á»ƒ Ä‘á»‘i chiáº¿u tá»›i cÃ¡c tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trÆ°á»›c Ä‘Ã³. 

Ta cÅ©ng sáº½ sá»­ dá»¥ng mÃ´ hÃ¬nh embedding `bkai-foundation-models/vietnamese-bi-encoder` Ä‘á»ƒ Ä‘áº£m báº£o cáº£ cÃ¢u há»i vÃ  tÃ i liá»‡u Ä‘Æ°á»£c truy váº¥n trÃªn cÃ¹ng má»™t há»‡ quy chiáº¿u. 


Trong Ä‘oáº¡n code sau: 
- CÃ¢u há»i Ä‘Æ°á»£c truyá»n qua itemgetter("question")
- Sau Ä‘Ã³ Ä‘i qua retriever (Ä‘Æ°á»£c táº¡o tá»« vector_db.as_retriever())
- Retriever sá»­ dá»¥ng cÃ¹ng mÃ´ hÃ¬nh e`mbedding bkai-foundation-models/vietnamese-bi-encoder` Ä‘á»ƒ chuyá»ƒn cÃ¢u há»i thÃ nh vector 

```python
retriever = vector_db.as_retriever()
rag_chain = (
    {
        "context": itemgetter("question")
        | retriever  # <-- Táº¡i Ä‘Ã¢y cÃ¢u há»i Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh vector Ä‘á»ƒ tÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan
        ...
    }
    ...
)

```
</details>

<details>
<summary>BÆ°á»›c 2: Truy váº¥n â€“ TÃ¬m kiáº¿m cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan nháº¥t trong cÆ¡ sá»Ÿ dá»¯ liá»‡u (ChromaDB)</summary>

Tiáº¿p tá»¥c BÆ°á»›c 1 Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ trÃªn: 
- Retriever tiáº¿p tá»¥c tÃ¬m kiáº¿m cÃ¡c tÃ i liá»‡u tÆ°Æ¡ng tá»± trong vector database.
- Vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh, retriever sáº½ tráº£ vá» 4 chunks (documents) cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t vá»›i cÃ¢u há»i.


</details>


<details>
<summary>BÆ°á»›c 3: Táº¡o prompt â€“ Káº¿t há»£p cÃ¢u há»i vÃ  Ä‘oáº¡n vÄƒn báº£n thÃ nh má»™t prompt hoÃ n chá»‰nh </summary>

Sau khi tá»•ng há»£p cÃ¡c dá»¯ liá»‡u cáº§n thiáº¿t (context data), chÃºng ta sáº½ tiáº¿n hÃ nh táº¡o Prompt Ä‘á»ƒ gá»Ÿi cho LLM. 

```python
     rag_chain = (
        {
            "context": itemgetter("question")
            | retriever
            | promptManager.format_docs_chunks,
            "question": itemgetter("question"),
            "chat_history": lambda x: promptManager.format_conversation_history(
                x["chat_history"]
            ),
        }
        | prompt # <-- táº¡o prompt dá»±a theo cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i Ä‘Ã£ truy váº¥n Ä‘Æ°á»£c. 
        | st.session_state.llm
        | StrOutputParser()
    )
```

Trong giáº£i phÃ¡p hiá»‡n táº¡i ta sá»­ dá»¥ng prompt template sau: 
```yaml
You are an assistant for question-answering tasks. Use the following pieces of retrieved context and conversation history to answer the question. If you don't know the answer, just say that you don't know. 
Instructions:
- Use three sentences maximum
- Keep the answer concise

Conversation history:
{chat_history}

Context:
{context} 

Question: {question} 

Answer:
```

</details>


<details>
<summary>BÆ°á»›c 4: Táº¡o sinh â€“ Dá»±a vÃ o prompt Ä‘Ã£ tÄƒng cÆ°á»ng Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng </summary>

Trong giáº£i phÃ¡p hiá»‡n táº¡i ta:
- Sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ `lmsys/vicuna-7b-v1.5`
- VÃ¬ giá»›i máº·t vá» máº·t pháº§n cá»©ng ta Ã¡p dá»¥ng quantizing vá» khÃ´ng gian 4bit Ä‘á»ƒ giáº£m yÃªu cáº§u vá» memory cá»§a GPU. 

```python
# Sá»­ dá»¥ng mÃ´ hÃ¬nh lmsys/vicuna-7b-v1.5  
def load_llm():
  MODEL_NAME = "lmsys/vicuna-7b-v1.5"
  nf4_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
  )
  model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=nf4_config, low_cpu_mem_usage = True
  )
  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
  model_pipeline = pipeline(
    "text-generation",
    model = model,
    tokenizer = tokenizer,
    max_new_tokens = 512,
    pad_token_id = tokenizer.eos_token_id,
    device_map = "auto"
  )
  return HuggingFacePipeline(pipeline = model_pipeline)
```
</details>



# 3. Thá»±c hiá»‡n âš™ 

á»¨ng dá»¥ng Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng Python vá»›i giao diá»‡n ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c Ä‘Æ°á»£c táº¡o bá»Ÿi thÆ° viá»‡n Streamlit. CÃ¡c thÆ° viá»‡n chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng bao gá»“m:
- Streamlit: XÃ¢y dá»±ng giao diá»‡n web cho á»©ng dá»¥ng.
- LangChain: Framework chÃ­nh Ä‘á»ƒ káº¿t ná»‘i cÃ¡c thÃ nh pháº§n trong chuá»—i RAG.
- Hugging Face Transformers: Táº£i vÃ  váº­n hÃ nh cÃ¡c mÃ´ hÃ¬nh embedding vÃ  LLM.
- ChromaDB: XÃ¢y dá»±ng cÆ¡ sá»Ÿ dá»¯ liá»‡u vector.
- PyPDF: Xá»­ lÃ½ file PDF.

Giao diá»‡n á»©ng dá»¥ng cho phÃ©p ngÆ°á»i dÃ¹ng:
- Táº£i lÃªn má»™t file tÃ i liá»‡u PDF.
- Nháº¥n nÃºt "Xá»­ lÃ½ PDF" Ä‘á»ƒ khá»Ÿi táº¡o quy trÃ¬nh láº­p chá»‰ má»¥c.
- Nháº­p cÃ¢u há»i vÃ o má»™t khung chat.
- Nháº­n cÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o ra bá»Ÿi há»‡ thá»‘ng.

Äá»ƒ tá»‘i Æ°u hÃ³a tráº£i nghiá»‡m, cÃ¡c mÃ´ hÃ¬nh náº·ng (embedding vÃ  LLM) Ä‘Æ°á»£c cache láº¡i báº±ng @st.cache_resource cá»§a Streamlit, Ä‘áº£m báº£o chÃºng chá»‰ cáº§n táº£i má»™t láº§n duy nháº¥t khi khá»Ÿi Ä‘á»™ng á»©ng dá»¥ng



# 4. Káº¿t quáº£ ğŸ“ˆ 

File code hoÃ n chá»‰nh vÃ  hÃ¬nh áº£nh giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vÃ  káº¿t quáº£ chatbot báº±ng RAG Ä‘Æ°á»£c ghi nháº­n sau Ä‘Ã¢y.

**ğŸ‘‰ Xem file code**: https://github.com/aio25-mix002/m01-p0102


![Táº£i model](/AIO.github.io/images/M01/M1-1.png)

HÃ¬nh 4.1: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Táº£i model.


![Táº£i file](/AIO.github.io/images/M01/M1-2.png)

HÃ¬nh 4.2: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Model Ä‘Ã£ sáºµn sÃ ng vÃ  táº£i file.


![Xá»­ lÃ½ file](/AIO.github.io/images/M01/M1-3.png)

HÃ¬nh 4.3: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Xá»­ lÃ½ file.


![Chatbot tráº£ lá»i](/AIO.github.io/images/M01/M1-5.png)

HÃ¬nh 4.4: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Äáº·t cÃ¢u há»i vÃ  chatbot tráº£ lá»i.


# 5. Má»Ÿ rá»™ng nÃ¢ng cao ğŸ–¥

Äiá»ƒm cáº£i tiáº¿n sau khi thá»±c hiá»‡n dá»± Ã¡n Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° nhau:

## 5.1 TiÃªu chÃ­ cáº£i tiáº¿n

| TiÃªu chÃ­                                   | PhiÃªn báº£n cÅ©                                                                | PhiÃªn báº£n cáº£i tiáº¿n                                                                                                    |
| ------------------------------------------ | --------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| **TÃ­nh nÄƒng: Ghi nhá»›**                     | âŒ KhÃ´ng ghi nhá»› há»™i thoáº¡i trÆ°á»›c Ä‘Ã³. Má»—i cÃ¢u há»i Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»™c láº­p.         | âœ… Ghi nhá»› vÃ  sá»­ dá»¥ng lá»‹ch sá»­ trÃ² chuyá»‡n Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c vÃ  máº¡ch láº¡c trong há»™i thoáº¡i.                             |
| **TÃ­nh nÄƒng: LÃ m viá»‡c vá»›i nhiá»u tÃ i liá»‡u** | âŒ Chá»‰ lÃ m viá»‡c vá»›i má»™t tÃ i liá»‡u                                             | âœ… LÃ m viá»‡c Ä‘Æ°á»£c vá»›i nhiá»u tÃ i liá»‡u                                                                                    |
| **Prompt: Prompt template**                | âŒChá»‰ bao gá»“m: tÃ i liá»‡u + cÃ¢u há»i                                            | âœ…Bao gá»“m: tÃ i liá»‡u + cÃ¢u há»i + lá»‹ch sá»­ há»™i thoáº¡i                                                                      |
| **Prompt: Thiáº¿t káº¿ prompt**                | âŒSá»­ dá»¥ng prompt template máº·c Ä‘á»‹nh tá»« hub                                    | âœ…Tá»± viáº¿t prompt cho phÃ©p  xá»­ lÃ½ linh hoáº¡t hÆ¡n.                                                                        |
| **á»¨ng dá»¥ng thá»±c táº¿**                       | âŒPhÃ¹ há»£p vá»›i truy váº¥n Ä‘Æ¡n láº», khÃ´ng cáº§n bá»‘i cáº£nh trÆ°á»›c Ä‘Ã³.                  | âœ…ThÃ­ch há»£p cho cÃ¡c cuá»™c há»™i thoáº¡i nhiá»u lÆ°á»£t cáº§n hiá»ƒu ngá»¯ cáº£nh.                                                       |
| **Coding: Cáº¥u trÃºc code & Module hÃ³a**     | âŒMÃ£ nguá»“n Ä‘Æ¡n giáº£n, Ã­t tÃ¡ch module.                                         | âœ…Cáº¥u trÃºc rÃµ rÃ ng, chia module tá»‘t vá»›i cÃ¡c class riÃªng nhÆ° `logging_utils` vÃ  `prompt_utils`                          |
| **Coding: Quáº£n lÃ½ Vector DB**              | âŒSá»­ dá»¥ng ChromaDB theo máº·c Ä‘á»‹nh, dá»… lá»—i khi xá»­ lÃ½ nhiá»u file PDF liÃªn tiáº¿p. | âœ…DÃ¹ng `chromadb.PersistentClient` vÃ  `reset()` trÆ°á»›c khi xá»­ lÃ½ file má»›i giÃºp trÃ¡nh lá»—i vÃ  quáº£n lÃ½ tráº¡ng thÃ¡i tá»‘t hÆ¡n. |
| **Coding: Gá»¡ lá»—i (Debugging)**             | âŒKhÃ´ng cÃ³ logging.                                                          | âœ…CÃ³ tÃ­ch há»£p `logger` Ä‘á»ƒ ghi láº¡i thÃ´ng tin debug (vÃ­ dá»¥: cÃ¡c chunks Ä‘Æ°á»£c truy váº¥n), há»— trá»£ phÃ¡t triá»ƒn tá»‘t hÆ¡n.        |



##  5.2 Code nÃ¢ng cao

### 5.2.1 NÃ¢ng cáº¥p cá»‘t lá»—i: Ghi nhá»› lá»‹ch sá»­ há»™i thoáº¡i (Conversation memory) 
<details>
<summary>5.2.1.1. XÃ¢y dá»±ng prompt cÃ³ chá»©a lá»‹ch sá»­ há»™i thoáº¡i </summary>
Ta sá»­ dá»¥ng ká»¹ thuáº­t Prompting Ä‘á»ƒ Ä‘Æ°a lá»‹ch sá»­ há»™i thoáº¡i vÃ o cÃ¢u prompt

```yaml
You are an assistant for question-answering tasks. Use the following pieces of retrieved context and conversation history to answer the question. If you don't know the answer, just say that you don't know. 
Instructions:
- Use three sentences maximum
- Keep the answer concise

Conversation history:
{chat_history}

Context:
{context} 

Question: {question} 

Answer:
```
</details>


<details>
<summary>5.2.1.2. Äá»‹nh dáº¡ng vÃ  truy xuáº¥t lá»‹ch sá»­ chat</summary>
LÃ½ tÆ°á»Ÿng thÃ¬ ta cÃ³ thá»ƒ Ä‘Æ°a toÃ n Ä‘á»™ Ä‘oáº¡n há»™i thoáº¡i vÃ o prompt, tuy nhiÃªn viá»‡c nÃ y cÃ³ thá»ƒ gÃ¢y vÆ°á»£t quÃ¡ context windows mÃ  LLM model cÃ³ thá»ƒ há»— trá»£. 

Giáº£i phÃ¡p hiá»‡n táº¡i lÃ  Ã¡p dá»¥ng ká»¹ thuáº­t Ä‘Æ¡n giáº£n nháº¥t lÃ  láº¥y 10 tin nháº¯n gáº§n Ä‘Ã¢y nháº¥t. 
- Æ¯u Ä‘iá»ƒm: Dá»… triá»ƒn khai
- NhÆ°á»£c Ä‘iá»ƒm/hÆ°á»›ng cáº£i tiáº¿n: 
    - Váº¥n Ä‘á» vÆ°á»£t quÃ¡ context windows cÅ©ng cÃ³ thá»ƒ xáº£y ra
    - CÃ¡c tin nháº¯n quÃ¡ khá»© náº¿u khÃ´ng liÃªn quan Ä‘áº¿n cÃ¢u há»i hiá»‡n táº¡i cÅ©ng cÃ³ thá»ƒ gÃ¢y nhiá»…u vÃ  áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ Ä‘áº§u ra. 

```python

def retrieve_chat_history():
    message_threshold = 10
    return st.session_state.chat_history[-message_threshold:-1]

def format_history(histories):
    formatted_history = ""
    for msg in histories:
        role = "User" if msg["role"] == "user" else "Assistant"
        formatted_history += f"{role}: {msg['content']}\n\n"
    return formatted_history.strip()
```
</details>

<details>
<summary>5.2.1.3. Cáº­p nháº­t RAG Chain Ä‘á»ƒ xá»­ lÃ½ lá»‹ch sá»­ chat </summary>

```python
def process_pdf_updated_chain(retriever, llm):
    prompt = build_prompt_ragprompt_withhistory_en()
    rag_chain = (
        {
            "context": itemgetter("question") | retriever | format_docs,
            "question": itemgetter("question"),
            "chat_history": lambda x: format_history(x["chat_history"]) # <--- chat_history Ä‘Æ°á»£c Ä‘Æ°a vÃ o context data táº¡i Ä‘Ã¢y
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain
```
</details>


<details>
<summary>5.2.1.4. Cáº­p nháº­t cÃ¡ch gá»i RAG chain (main_updated_invoke) </summary>
```python
#HÃ m main_updated_invoke
def main_updated_invoke(user_input):
    output = st.session_state.rag_chain.invoke({
        "question": user_input,
        "chat_history": retrieve_chat_history()
    })
```
</details>

### 5.2.2 Quáº£n lÃ½ Vector DB nÃ¢ng cao
<details>
<summary>LÆ°u Vector DB xuá»‘ng á»• Ä‘Ä©a (persistence) Ä‘á»ƒ dá»… debug vÃ  trÃ¡nh cÃ¡c lá»—i trÃªn in-memory </summary>

```python
def get_chroma_client(allow_reset=False):
    """Get a Chroma client for vector database operations."""
    return chromadb.PersistentClient(settings=chromadb.Settings(allow_reset=allow_reset))

def process_pdf_updated_db_handling():
    client = get_chroma_client(allow_reset=True)
    client.reset()
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=st.session_state.embeddings,
        client=client
    )
```
</details>


### 5.2.3. Gá»¡ lá»—i (Debugging) vá»›i Logger
<details>
<summary>ThÃªm logger vÃ o á»©ng dá»¥ng Ä‘á»ƒ dá»… truy váº¿t </summary>

```python
def format_docs_with_logging(docs):
    logger.info(f"**Debug: Retrieved {len(docs)} chunks:**")
    for i, doc in enumerate(docs):
        page_num = doc.metadata.get('page') + 1 if 'page' in doc.metadata else -1
        source = doc.metadata.get('source', 'document')
        file_name = os.path.basename(source) if isinstance(source, str) else 'unknown'

        logger.info(f"""
        ([reference-{i+1}] Page {page_num} - Source: {file_name})
        {doc.page_content}""")
    
    return "\n\n".join(doc.page_content for doc in docs)
```
</details>

### 5.2.4. Xá»­ lÃ½ vÃ  truy váº¥n tá»« nhiá»u file tÃ i liá»‡u
<details>

```python
def process_pdf(uploaded_files):
    """Process multiple uploaded PDF files, combine their docs, and build a single retriever and RAG chain."""
    all_docs = []
    file_names = []
    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(
            delete=False, prefix=uploaded_file.name, suffix=".pdf"
        ) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        semantic_splitter = SemanticChunker(
            embeddings=st.session_state.embeddings,
            buffer_size=1,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95,
            min_chunk_size=500,
            add_start_index=True,
        )
        docs = semantic_splitter.split_documents(documents)
        # for doc in docs:
        #     # Add file name to doc metadata for citation
        #     doc.metadata["source"] = uploaded_file.name
        all_docs.extend(docs)
        file_names.append(uploaded_file.name)
        os.unlink(tmp_file_path)

    chroma_client = get_chroma_client(allow_reset=True)
    chroma_client.reset()  # empties and completely resets the database. This is destructive and not reversible.

    vector_db = Chroma.from_documents(
        documents=all_docs, embedding=st.session_state.embeddings, client=chroma_client
    )
    retriever = vector_db.as_retriever()
    # prompt = hub.pull("rlm/rag-prompt")
    prompt = promptManager.load_prompt_template_from_file("rag_with_memory.v1.txt")

    # Build the RAG chain
    # This use  LangChain Expression Language (LCEL) 
    # where  LangChain overrides the Python pipe operator (|) for its Runnable objects
    # Ref: https://python.langchain.com/docs/concepts/lcel/
    rag_chain = (
        {
            "context": itemgetter("question")
            | retriever
            | promptManager.format_docs_chunks,
            "question": itemgetter("question"),
            "chat_history": lambda x: promptManager.format_conversation_history(
                x["chat_history"]
            ),
        }
        | prompt
        | st.session_state.llm
        | StrOutputParser()
    )
    return rag_chain, len(all_docs), file_names

```
</details>

##  5.3 Káº¿t quáº£ má»Ÿ rá»™ng ğŸ“

HÃ¬nh áº£nh giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vÃ  káº¿t quáº£ chatbot báº±ng RAG sau khi cáº£i tiáº¿n Ä‘Æ°á»£c ghi nháº­n sau Ä‘Ã¢y.

### 5.3.1 Há»— trá»£ ghi nhá»› 
![Data máº«u YOLOv10_Tutorials](/AIO.github.io/images/M01/M1-6.png)

HÃ¬nh 5: Káº¿t quáº£ giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vá»›i file Data máº«u YOLOv10_Tutorials.pdf


### 5.3.2 Xá»­ dá»¥ng táº­p tÃ i liá»‡u khÃ¡c á»©ng dá»¥ng trong y khoa
![file Medical Report](/AIO.github.io/images/M01/M1-7.png)

HÃ¬nh 6: Káº¿t quáº£ giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vá»›i file Medical Report

### 5.3.3 Há»— trá»£ lÃ m viá»‡c vá»›i nhiá»u tÃ i liá»‡u khÃ¡c nhau
![file Multiple File](/AIO.github.io/images/M01/M1-8.jpg)

HÃ¬nh 7: Káº¿t quáº£ giao diá»‡n lÃ m viá»‡c vá»›i nhiá»u tÃ i liá»‡u khÃ¡c nhau

# 6. Káº¿t luáº­n ğŸ“Œ 

- Dá»± Ã¡n Ä‘Ã£ **xÃ¢y dá»±ng thÃ nh cÃ´ng má»™t chatbot á»©ng dá»¥ng kiáº¿n trÃºc RAG, cÃ³ kháº£ nÄƒng há»i Ä‘Ã¡p trá»±c tiáº¿p vÃ  hiá»‡u quáº£ vá»›i cÃ¡c tÃ i liá»‡u PDF chuyÃªn biá»‡t**, phÃ¹ há»£p vá»›i ngá»¯ cáº£nh báº±ng cÃ¡ch káº¿t há»£p truy váº¥n thÃ´ng tin cá»§a cÆ¡ sá»Ÿ dá»¯ liá»‡u vector vÃ  kháº£ nÄƒng táº¡o sinh ngÃ´n ngá»¯ cá»§a LLMs.
  
- Cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i cá»§a há»‡ thá»‘ng **phá»¥ thuá»™c hoÃ n toÃ n vÃ o hiá»‡u quáº£ cá»§a bÆ°á»›c truy váº¥n thÃ´ng tin (retrieval)**. Náº¿u quÃ¡ trÃ¬nh tÃ¬m kiáº¿m ngá»¯ nghÄ©a khÃ´ng tÃ¬m Ä‘Æ°á»£c Ä‘Ãºng Ä‘oáº¡n vÄƒn báº£n chá»©a thÃ´ng tin liÃªn quan trong Vector Database, mÃ´ hÃ¬nh LLM sáº½ khÃ´ng cÃ³ Ä‘á»§ ngá»¯ cáº£nh cáº§n thiáº¿t, dáº«n Ä‘áº¿n nguy cÆ¡ táº¡o ra cÃ¢u tráº£ lá»i sai, khÃ´ng Ä‘áº§y Ä‘á»§ hoáº·c khÃ´ng liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.
  
- Vá»›i phÆ°Æ¡ng phÃ¡p nÃ y, dá»± Ã¡n má»Ÿ ra nhiá»u hÆ°á»›ng phÃ¡t triá»ƒn tiá»m nÄƒng trong tÆ°Æ¡ng lai Ä‘á»ƒ **tiáº¿p tá»¥c tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™, Ä‘á»™ chÃ­nh xÃ¡c vÃ  nÃ¢ng cao tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng**.
