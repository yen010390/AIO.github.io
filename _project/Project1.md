---
title: "Module 1: á»¨ng dá»¥ng RAG trong viá»‡c há»i Ä‘Ã¡p tÃ i liá»‡u bÃ i há»c AIO"
excerpt: "Dá»± Ã¡n nÃ y xÃ¢y dá»±ng há»‡ thá»‘ng há»i Ä‘Ã¡p thÃ´ng minh dÃ¹ng kiáº¿n trÃºc RAG, giÃºp ngÆ°á»i há»c khÃ³a AI táº¡i AI Viá»‡t Nam (AIO) khai thÃ¡c hiá»‡u quáº£ ná»™i dung tÃ i liá»‡u há»c táº­p."
collection: project
author: "Nguyá»…n Tuáº¥n Anh - ÄoÃ n Táº¥n HÆ°ng - Há»“ Thá»‹ Ngá»c Huyá»n - Tráº§n Thá»‹ Má»¹ TÃº - Äáº·ng Thá»‹ HoÃ ng Yáº¿n"
---

TÃ¡c giáº£: Nguyá»…n Tuáº¥n Anh - ÄoÃ n Táº¥n HÆ°ng - Há»“ Thá»‹ Ngá»c Huyá»n - Tráº§n Thá»‹ Má»¹ TÃº - Äáº·ng Thá»‹ HoÃ ng Yáº¿n

<details>
<summary><strong>ğŸ“ Xem Cáº¥u trÃºc thÆ° má»¥c dá»± Ã¡n</strong></summary>

```text
ğŸ“¦RAG_AIO_Chatbot
â”œâ”€â”€ ğŸ“data/                       # ThÆ° má»¥c chá»©a cÃ¡c file PDF Ä‘áº§u vÃ o
â”‚   â”œâ”€â”€ YOLOv10_Tutorials.pdf
â”‚   â””â”€â”€ Medical_Report.pdf
â”‚
â”œâ”€â”€ ğŸ“utils/
â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”œâ”€â”€ history_utils.py
â”‚   â”œâ”€â”€ prompt_utils.py
â”‚   â””â”€â”€ logger_utils.py
â”‚
â”œâ”€â”€ ğŸ“models/
â”‚   â”œâ”€â”€ embedding_loader.py
â”‚   â””â”€â”€ llm_loader.py
â”‚
â”œâ”€â”€ ğŸ“files/
â”‚   â”œâ”€â”€ M01_rag_chatbot.py
â”‚   â”œâ”€â”€ M01_rag_chatbot_cai_tien.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml
```
</details>

<details>
<summary><strong>ğŸ“ Xem Má»¥c lá»¥c bÃ¡o cÃ¡o</strong></summary>

```text
ğŸ“¦RAG_AIO_Chatbot
â”œâ”€â”€ ğŸ“ TÃ³m táº¯t
â”‚
â”œâ”€â”€ ğŸ—‚ 1. Giá»›i thiá»‡u 
â”‚
â”œâ”€â”€ ğŸ“š 2. PhÆ°Æ¡ng phÃ¡p luáº­n 
â”‚   â”œâ”€â”€ 2.1. Quy trÃ¬nh Láº­p chá»‰ má»¥c dá»¯ liá»‡u (Indexing)
â”‚   â””â”€â”€ 2.2. Quy trÃ¬nh Truy váº¥n vÃ  Táº¡o sinh (Retrieval & Generation)
â”‚
â”œâ”€â”€ âš™ 3. Thá»±c hiá»‡n  
â”œâ”€â”€ ğŸ“ˆ 4. Káº¿t quáº£  
â”œâ”€â”€ ğŸ–¥ 5. Má»Ÿ rá»™ng nÃ¢ng cao
â”‚   â”œâ”€â”€ 5.1 TiÃªu chÃ­ cáº£i tiáº¿n
â”‚   â”œâ”€â”€ 5.2 Code nÃ¢ng cao
â”‚   â””â”€â”€ 5.3 Káº¿t quáº£ má»Ÿ rá»™ng
â””â”€â”€ ğŸ“Œ 6. Káº¿t luáº­n 
```
</details>


# TÃ³m táº¯t
Máº·c dÃ¹ LLMs ráº¥t máº¡nh, chÃºng váº«n bá»‹ háº¡n cháº¿ vá» kiáº¿n thá»©c chuyÃªn ngÃ nh vÃ  tÃ­nh cáº­p nháº­t. Dá»± Ã¡n nÃ y xÃ¢y dá»±ng há»‡ thá»‘ng há»i Ä‘Ã¡p thÃ´ng minh dÃ¹ng kiáº¿n trÃºc RAG, giÃºp ngÆ°á»i há»c khÃ³a AI táº¡i AI Viá»‡t Nam (AIO) khai thÃ¡c hiá»‡u quáº£ ná»™i dung tÃ i liá»‡u há»c táº­p.


# 1. Giá»›i thiá»‡u ğŸ—‚ 
- CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs) nhÆ° ChatGPT cÃ³ kháº£ nÄƒng tráº£ lá»i linh hoáº¡t nhÆ°ng bá»‹ giá»›i háº¡n bá»Ÿi dá»¯ liá»‡u huáº¥n luyá»‡n, nÃªn khÃ´ng xá»­ lÃ½ tá»‘t thÃ´ng tin má»›i hoáº·c cÃ¡ nhÃ¢n hÃ³a.
- Äá»ƒ kháº¯c phá»¥c, kiáº¿n trÃºc Retrieval-Augmented Generation (RAG) cho phÃ©p LLM truy xuáº¥t thÃ´ng tin tá»« nguá»“n ngoÃ i (nhÆ° PDF, cÆ¡ sá»Ÿ dá»¯ liá»‡u) trÆ°á»›c khi táº¡o cÃ¢u tráº£ lá»i, giÃºp káº¿t quáº£ chÃ­nh xÃ¡c vÃ  phÃ¹ há»£p hÆ¡n.
- Má»¥c tiÃªu dá»± Ã¡n lÃ  xÃ¢y dá»±ng chatbot á»©ng dá»¥ng RAG, há»— trá»£ há»c viÃªn khÃ³a AIO há»i â€“ Ä‘Ã¡p trá»±c tiáº¿p dá»±a trÃªn ná»™i dung tÃ i liá»‡u bÃ i giáº£ng.

# 2. PhÆ°Æ¡ng phÃ¡p luáº­n ğŸ“š 
Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc RAG tiÃªu chuáº©n, bao gá»“m hai quy trÃ¬nh chÃ­nh: Láº­p chá»‰ má»¥c dá»¯ liá»‡u (Indexing) vÃ  Truy váº¥n & Táº¡o sinh (Retrieval & Generation).

![Quy trÃ¬nh RAG tá»•ng quan](/AIO.github.io/images/M01/M01_RAG_1.png)

HÃ¬nh 1: SÆ¡ Ä‘á»“ tá»•ng quan vá» chÆ°Æ¡ng trÃ¬nh RAG trong project.


## 2.1. Quy trÃ¬nh Láº­p chá»‰ má»¥c dá»¯ liá»‡u (Indexing)

<details>
<summary>BÆ°á»›c 1: Táº£i dá»¯ liá»‡u â€“ Äá»c vÃ  trÃ­ch xuáº¥t vÄƒn báº£n tá»« file PDF (PyPDFLoader) </summary>
<pre><code class="language-python">
#HÃ m PyPDFLoader
from langchain.document_loaders import PyPDFLoader

# Táº£i file PDF vÃ  trÃ­ch xuáº¥t vÄƒn báº£n
loader = PyPDFLoader(tmp_file_path)
documents = loader.load()
</code></pre>
</details>

<details>
<summary>BÆ°á»›c 2: PhÃ¢n Ä‘oáº¡n â€“ Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunks) cÃ³ Ã½ nghÄ©a (SemanticChunker) </summary>
<pre><code class="language-python">
#HÃ m SemanticChunker
from langchain.text_splitter import SemanticChunker

semantic_splitter = SemanticChunker(
    embeddings = st.session_state.embeddings,
    buffer_size = 1,
    breakpoint_threshold_type = "percentile",
    breakpoint_threshold_amount = 95,
    min_chunk_size = 500,
    add_start_index = True
)
</code></pre>
</details>

![Semantic Chunking](/AIO.github.io/images/M01/M01_RAG_3.png)

HÃ¬nh 2: SÆ¡ Ä‘á»“ vá» Semantic Chunking.

<details>
<summary>BÆ°á»›c 3: MÃ£ hÃ³a â€“ Chuyá»ƒn má»—i Ä‘oáº¡n vÄƒn báº£n thÃ nh vector sá»‘ há»c (bkai-foundation-models/vietnamese-bi-encoder) </summary>

<pre><code class="language-python">
#HÃ m bkai-foundation-models/vietnamese-bi-encoder
from langchain.embeddings import HuggingFaceEmbeddings

def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")
</code></pre>
</details>

![Vector database](/AIO.github.io/images/M01/M01_RAG_2.png)

HÃ¬nh 3: SÆ¡ Ä‘á»“ bÆ°á»›c thá»±c hiá»‡n xÃ¢y dá»±ng vector database.


<details>
<summary>BÆ°á»›c 4: LÆ°u trá»¯ â€“ LÆ°u cÃ¡c vector vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ truy váº¥n nhanh (langchain.vectorstores, Chroma) </summary>
<pre><code class="language-python">    
from langchain.vectorstores import Chroma

#ChromaDB, langchain.vectorstores

# PhÃ¢n Ä‘oáº¡n vÃ  lÆ°u trá»¯ vector
docs = semantic_splitter.split_documents(documents)
vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
retriever = vector_db.as_retriever()

# Táº£i prompt máº«u tá»« hub
from langchain import hub
prompt = hub.pull("rlm/rag-prompt")
</code></pre>
</details>


## 2.2. Quy trÃ¬nh Truy váº¥n vÃ  Táº¡o sinh (Retrieval & Generation)

<details>
<summary>BÆ°á»›c 1: MÃ£ hÃ³a cÃ¢u há»i â€“ Chuyá»ƒn cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng thÃ nh vector (bkai-foundation-models/vietnamese-bi-encode) </summary>
<pre><code class="language-python">
#HÃ m bkai-foundation-models/vietnamese-bi-encoder
@st.cache_resource
def load_embeddings():
  return HuggingFaceEmbeddings(model_name = "bkai-foundation-models/vietnamese-bi-encoder")
</code></pre>
</details>

<details>
<summary>BÆ°á»›c 2: Truy váº¥n â€“ TÃ¬m kiáº¿m cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan nháº¥t trong cÆ¡ sá»Ÿ dá»¯ liá»‡u (ChromaDB)</summary>
<pre><code class="language-python">
#HÃ m ChromaDB
vector_db = Chroma.from_documents(documents=docs,embedding=st.session_state.embeddings)
</code></pre>
</details>


<details>
<summary>BÆ°á»›c 3: TÄƒng cÆ°á»ng â€“ Káº¿t há»£p cÃ¢u há»i vÃ  Ä‘oáº¡n vÄƒn báº£n thÃ nh má»™t prompt hoÃ n chá»‰nh (rlm/rag-prompt) </summary>    
<pre><code class="language-python">
 rlm/rag-prompt
</code></pre>
</details>


<details>
<summary>BÆ°á»›c 4: Táº¡o sinh â€“ Dá»±a vÃ o prompt Ä‘Ã£ tÄƒng cÆ°á»ng Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng (lmsys/vicuna-7b-v1.5) </summary>
<pre><code class="language-python">
#HÃ m lmsys/vicuna-7b-v1.5  
def load_llm():
  MODEL_NAME = "lmsys/vicuna-7b-v1.5"
  nf4_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
  )
  model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=nf4_config, low_cpu_mem_usage = True
  )
  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
  model_pipeline = pipeline(
    "text-generation",
    model = model,
    tokenizer = tokenizer,
    max_new_tokens = 512,
    pad_token_id = tokenizer.eos_token_id,
    device_map = "auto"
  )
  return HuggingFacePipeline(pipeline = model_pipeline)
</code></pre>
</details>


# 3. Thá»±c hiá»‡n âš™ 

á»¨ng dá»¥ng Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng Python vá»›i giao diá»‡n ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c Ä‘Æ°á»£c táº¡o bá»Ÿi thÆ° viá»‡n Streamlit. CÃ¡c thÆ° viá»‡n chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng bao gá»“m:
- Streamlit: XÃ¢y dá»±ng giao diá»‡n web cho á»©ng dá»¥ng.
- LangChain: Framework chÃ­nh Ä‘á»ƒ káº¿t ná»‘i cÃ¡c thÃ nh pháº§n trong chuá»—i RAG.
- Hugging Face Transformers: Táº£i vÃ  váº­n hÃ nh cÃ¡c mÃ´ hÃ¬nh embedding vÃ  LLM.
- ChromaDB: XÃ¢y dá»±ng cÆ¡ sá»Ÿ dá»¯ liá»‡u vector.
- PyPDF: Xá»­ lÃ½ file PDF.

Giao diá»‡n á»©ng dá»¥ng cho phÃ©p ngÆ°á»i dÃ¹ng:
- Táº£i lÃªn má»™t file tÃ i liá»‡u PDF.
- Nháº¥n nÃºt "Xá»­ lÃ½ PDF" Ä‘á»ƒ khá»Ÿi táº¡o quy trÃ¬nh láº­p chá»‰ má»¥c.
- Nháº­p cÃ¢u há»i vÃ o má»™t khung chat.
- Nháº­n cÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o ra bá»Ÿi há»‡ thá»‘ng.

Äá»ƒ tá»‘i Æ°u hÃ³a tráº£i nghiá»‡m, cÃ¡c mÃ´ hÃ¬nh náº·ng (embedding vÃ  LLM) Ä‘Æ°á»£c cache láº¡i báº±ng @st.cache_resource cá»§a Streamlit, Ä‘áº£m báº£o chÃºng chá»‰ cáº§n táº£i má»™t láº§n duy nháº¥t khi khá»Ÿi Ä‘á»™ng á»©ng dá»¥ng

# 4. Káº¿t quáº£ ğŸ“ˆ 

File code hoÃ n chá»‰nh vÃ  hÃ¬nh áº£nh giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vÃ  káº¿t quáº£ chatbot báº±ng RAG Ä‘Æ°á»£c ghi nháº­n sau Ä‘Ã¢y.

[ğŸ‘‰ Xem file code](/AIO.github.io/files/M01_rag_chatbot.py)


![Táº£i model](/AIO.github.io/images/M01/M1-1.png)

HÃ¬nh 4.1: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Táº£i model.


![Táº£i file](/AIO.github.io/images/M01/M1-2.png)

HÃ¬nh 4.2: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Model Ä‘Ã£ sáºµn sÃ ng vÃ  táº£i file.


![Xá»­ lÃ½ file](/AIO.github.io/images/M01/M1-3.png)

HÃ¬nh 4.3: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Xá»­ lÃ½ file.


![Chatbot tráº£ lá»i](/AIO.github.io/images/M01/M1-5.png)

HÃ¬nh 4.4: Giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng - Äáº·t cÃ¢u há»i vÃ  chatbot tráº£ lá»i.


# 5. Má»Ÿ rá»™ng nÃ¢ng cao ğŸ–¥

Äiá»ƒm cáº£i tiáº¿n sau khi thá»±c hiá»‡n dá»± Ã¡n Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° nhau:

## **5.1 TiÃªu chÃ­ cáº£i tiáº¿n:**

| TiÃªu chÃ­                     | PhiÃªn báº£n cÅ©                                                                                           | PhiÃªn báº£n cáº£i tiáº¿n                                                                                                   |
|-----------------------------|----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| **Kháº£ nÄƒng ghi nhá»›**        | âŒ KhÃ´ng ghi nhá»› há»™i thoáº¡i trÆ°á»›c Ä‘Ã³. Má»—i cÃ¢u há»i Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»™c láº­p.                                                       | âœ… Ghi nhá»› vÃ  sá»­ dá»¥ng lá»‹ch sá»­ trÃ² chuyá»‡n Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c vÃ  máº¡ch láº¡c trong há»™i thoáº¡i.                                             |
| **CÃ¡ch gá»i `rag_chain`**    | Gá»i vá»›i **chá»‰ cÃ¢u há»i**:<br>`rag_chain.invoke(user_input)`                                                                 | Gá»i vá»›i **cÃ¢u há»i + lá»‹ch sá»­ há»™i thoáº¡i**:<br>`rag_chain.invoke({ "question": user_input, "chat_history": retrieve_chat_history() })`   |
| **Thiáº¿t káº¿ prompt**         | Sá»­ dá»¥ng prompt máº·c Ä‘á»‹nh tá»« hub, khÃ´ng cÃ³ thÃ´ng tin há»™i thoáº¡i trÆ°á»›c Ä‘Ã³.                                                     | Prompt há»— trá»£ tÃ­ch há»£p lá»‹ch sá»­ há»™i thoáº¡i, tÃ¹y chá»n tiáº¿ng Viá»‡t/Anh, cho phÃ©p thá»­ nghiá»‡m linh hoáº¡t hÆ¡n.                                |
| **á»¨ng dá»¥ng thá»±c táº¿**        | PhÃ¹ há»£p vá»›i truy váº¥n Ä‘Æ¡n láº», khÃ´ng cáº§n bá»‘i cáº£nh trÆ°á»›c Ä‘Ã³.                                                                  | ThÃ­ch há»£p cho cÃ¡c cuá»™c há»™i thoáº¡i nhiá»u lÆ°á»£t cáº§n hiá»ƒu ngá»¯ cáº£nh.                                                                        |
| **Cáº¥u trÃºc & Module hÃ³a**   | MÃ£ nguá»“n Ä‘Æ¡n giáº£n, Ã­t tÃ¡ch module.                                                                                         | Cáº¥u trÃºc rÃµ rÃ ng, chia module tá»‘t vá»›i cÃ¡c hÃ m riÃªng nhÆ° `build_prompt_...`, `get_chroma_client()`.                                    |
| **Quáº£n lÃ½ Vector DB**       | Sá»­ dá»¥ng ChromaDB theo máº·c Ä‘á»‹nh, dá»… lá»—i khi xá»­ lÃ½ nhiá»u file PDF liÃªn tiáº¿p.                                                  | DÃ¹ng `chromadb.PersistentClient` vÃ  `reset()` trÆ°á»›c khi xá»­ lÃ½ file má»›i giÃºp trÃ¡nh lá»—i vÃ  quáº£n lÃ½ tráº¡ng thÃ¡i tá»‘t hÆ¡n.                 |
| **Ká»¹ thuáº­t Prompt**         | Duy nháº¥t má»™t prompt tá»« `hub.pull("rlm/rag-prompt")`.                                                                      | CÃ³ nhiá»u tÃ¹y chá»n prompt: tiáº¿ng Viá»‡t, tiáº¿ng Anh, tÃ­ch há»£p lá»‹ch sá»­ há»™i thoáº¡i.                                                         |
| **Giao diá»‡n ngÆ°á»i dÃ¹ng (UI)**| Giao diá»‡n Ä‘Æ¡n giáº£n.                                                                                                        | CÃ³ nÃºt "XÃ³a lá»‹ch sá»­ chat", hiá»ƒn thá»‹ logo (`st.logo`), cÃ¡c nÃºt sá»­ dá»¥ng `use_container_width=True` giÃºp UI gá»n gÃ ng, hiá»‡n Ä‘áº¡i hÆ¡n.     |
| **Gá»¡ lá»—i (Debugging)**      | KhÃ´ng cÃ³ logging.                                                                                                           | CÃ³ tÃ­ch há»£p `logger` Ä‘á»ƒ ghi láº¡i thÃ´ng tin debug (vÃ­ dá»¥: cÃ¡c chunks Ä‘Æ°á»£c truy váº¥n), há»— trá»£ phÃ¡t triá»ƒn tá»‘t hÆ¡n.                        |
| **ThÆ° viá»‡n phá»¥ thuá»™c**      | Ãt thÆ° viá»‡n hÆ¡n.                                                                                                            | ThÃªm thÆ° viá»‡n nhÆ° `chromadb`, `ChatPromptTemplate`, `itemgetter` vÃ  module `utils` tÃ¹y chá»‰nh.                                         |


##  5.2 Code nÃ¢ng cao

File code cáº£i tiáº¿n vÃ  nhá»¯ng hÃ m sá»­ dá»¥ng thÃªm trong bÃ¡o cÃ¡o Ä‘Æ°á»£c liá»‡t kÃª sau Ä‘Ã¢y.
[ğŸ‘‰ Xem file code cáº£i tiáº¿n](/AIO.github.io/files/M01_rag_chatbot_cai_tien.py)

### 5.2.1 NÃ¢ng cáº¥p cá»‘t lá»—i: Ghi nhá»› lá»‹ch sá»­ há»™i thoáº¡i (Conversation memory) 
<details>
<summary>5.2.1.1. HÃ m xÃ¢y dá»±ng prompt cÃ³ chá»©a lá»‹ch sá»­ há»™i thoáº¡i (build_prompt_ragprompt_withhistory_en) </summary>
<pre><code class="language-python">
#HÃ m build_prompt_ragprompt_withhistory_en
def build_prompt_ragprompt_withhistory_en():
    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context and conversation history to answer the question. If you don't know the answer, just say that you don't know. 
    Instructions:
    - Use three sentences maximum
    - Keep the answer concise

    Conversation history:
    {chat_history}
    
    Context:
    {context} 

    Question: {question} 

    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt
</code></pre>
</details>


<details>
<summary>5.2.1.2. HÃ m Ä‘á»‹nh dáº¡ng vÃ  truy xuáº¥t lá»‹ch sá»­ chat (retrieve_chat_history, chat_history)</summary>
<pre><code class="language-python">
#HÃ m retrieve_chat_history, format_history
def retrieve_chat_history():
    message_threshold = 10
    return st.session_state.chat_history[-message_threshold:] if len(st.session_state.chat_history) >= message_threshold else st.session_state.chat_history

def format_history(histories):
    formatted_history = ""
    for msg in histories:
        role = "User" if msg["role"] == "user" else "Assistant"
        formatted_history += f"{role}: {msg['content']}\n\n"
    return formatted_history.strip()
</code></pre>
</details>

<details>
<summary>5.2.1.3. Cáº­p nháº­t RAG Chain Ä‘á»ƒ xá»­ lÃ½ lá»‹ch sá»­ chat (process_pdf_updated_chain(retriever, llm)) </summary>
<pre><code class="language-python">
#HÃ m process_pdf_updated_chain(retriever, llm)
def process_pdf_updated_chain(retriever, llm):
    prompt = build_prompt_ragprompt_withhistory_en()
    rag_chain = (
        {
            "context": itemgetter("question") | retriever | format_docs,
            "question": itemgetter("question"),
            "chat_history": lambda x: format_history(x["chat_history"])
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain
</code></pre>
</details>


<details>
<summary>5.2.1.4. Cáº­p nháº­t cÃ¡ch gá»i RAG chain (main_updated_invoke) </summary>
<pre><code class="language-python">
#HÃ m main_updated_invoke
def main_updated_invoke(user_input):
    output = st.session_state.rag_chain.invoke({
        "question": user_input,
        "chat_history": retrieve_chat_history()
    })
</code></pre>
</details>

### 5.2.2 Quáº£n lÃ½ Vector DB nÃ¢ng cao
<details>
<summary>XÃ¢y dá»±ng hÃ m get_chroma_client, process_pdf_updated_db_handling </summary>
<pre><code class="language-python">
def get_chroma_client(allow_reset=False):
    """Get a Chroma client for vector database operations."""
    return chromadb.PersistentClient(settings=chromadb.Settings(allow_reset=allow_reset))

def process_pdf_updated_db_handling():
    client = get_chroma_client(allow_reset=True)
    client.reset()
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=st.session_state.embeddings,
        client=client
    )
</code></pre>
</details>


### 5.2.3. Gá»¡ lá»—i (Debugging) vá»›i Logger
<details>
<summary>XÃ¢y dá»±ng hÃ m format_docs_with_logging </summary>

<pre><code class="language-python">
def format_docs_with_logging(docs):
    logger.info(f"**Debug: Retrieved {len(docs)} chunks:**")
    for i, doc in enumerate(docs):
        page_num = doc.metadata.get('page') + 1 if 'page' in doc.metadata else -1
        source = doc.metadata.get('source', 'document')
        file_name = os.path.basename(source) if isinstance(source, str) else 'unknown'

        logger.info(f"""
        ([reference-{i+1}] Page {page_num} - Source: {file_name})
        {doc.page_content}""")
    
    return "\n\n".join(doc.page_content for doc in docs)
</code></pre>
</details>

### 5.2.4. Cáº£i tiáº¿n giao diá»‡n ngÆ°á»i dÃ¹ng (UI)
<details>
<summary>XÃ¢y dá»±ng hÃ m  main_sidebar_enhancements </summary>
<pre><code class="language-python">
def main_sidebar_enhancements():
    with st.sidebar:
        st.logo("./assets/logo.png")
        st.subheader("ğŸ’¬ Äiá»u khiá»ƒn Chat")
        if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat", use_container_width=True):
            clear_chat()
            st.rerun()
</code></pre>
</details>

##  5.3 Káº¿t quáº£ má»Ÿ rá»™ng ğŸ“

HÃ¬nh áº£nh giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vÃ  káº¿t quáº£ chatbot báº±ng RAG sau khi cáº£i tiáº¿n Ä‘Æ°á»£c ghi nháº­n sau Ä‘Ã¢y.

![Data máº«u YOLOv10_Tutorials](/AIO.github.io/images/M01/M1-6.png)

HÃ¬nh 5: Káº¿t quáº£ giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vá»›i file Data máº«u YOLOv10_Tutorials.pdf


![file Medical Report](/AIO.github.io/images/M01/M1-7.png)

HÃ¬nh 6: Káº¿t quáº£ giao diá»‡n cá»§a ngÆ°á»i dÃ¹ng vá»›i file Medical Report

# 6. Káº¿t luáº­n ğŸ“Œ 

- Dá»± Ã¡n Ä‘Ã£ **xÃ¢y dá»±ng thÃ nh cÃ´ng má»™t chatbot á»©ng dá»¥ng kiáº¿n trÃºc RAG, cÃ³ kháº£ nÄƒng há»i Ä‘Ã¡p trá»±c tiáº¿p vÃ  hiá»‡u quáº£ vá»›i cÃ¡c tÃ i liá»‡u PDF chuyÃªn biá»‡t**, phÃ¹ há»£p vá»›i ngá»¯ cáº£nh báº±ng cÃ¡ch káº¿t há»£p truy váº¥n thÃ´ng tin cá»§a cÆ¡ sá»Ÿ dá»¯ liá»‡u vector vÃ  kháº£ nÄƒng táº¡o sinh ngÃ´n ngá»¯ cá»§a LLMs.
  
- Cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i cá»§a há»‡ thá»‘ng **phá»¥ thuá»™c hoÃ n toÃ n vÃ o hiá»‡u quáº£ cá»§a bÆ°á»›c truy váº¥n thÃ´ng tin (retrieval)**. Náº¿u quÃ¡ trÃ¬nh tÃ¬m kiáº¿m ngá»¯ nghÄ©a khÃ´ng tÃ¬m Ä‘Æ°á»£c Ä‘Ãºng Ä‘oáº¡n vÄƒn báº£n chá»©a thÃ´ng tin liÃªn quan trong Vector Database, mÃ´ hÃ¬nh LLM sáº½ khÃ´ng cÃ³ Ä‘á»§ ngá»¯ cáº£nh cáº§n thiáº¿t, dáº«n Ä‘áº¿n nguy cÆ¡ táº¡o ra cÃ¢u tráº£ lá»i sai, khÃ´ng Ä‘áº§y Ä‘á»§ hoáº·c khÃ´ng liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.
  
- Vá»›i phÆ°Æ¡ng phÃ¡p nÃ y, dá»± Ã¡n má»Ÿ ra nhiá»u hÆ°á»›ng phÃ¡t triá»ƒn tiá»m nÄƒng trong tÆ°Æ¡ng lai Ä‘á»ƒ **tiáº¿p tá»¥c tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™, Ä‘á»™ chÃ­nh xÃ¡c vÃ  nÃ¢ng cao tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng**.
