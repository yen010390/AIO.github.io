---
title: "Module 1: ·ª®ng d·ª•ng RAG trong vi·ªác h·ªèi ƒë√°p t√†i li·ªáu b√†i h·ªçc AIO"
excerpt: "D·ª± √°n n√†y x√¢y d·ª±ng h·ªá th·ªëng h·ªèi ƒë√°p th√¥ng minh d√πng ki·∫øn tr√∫c RAG, gi√∫p ng∆∞·ªùi h·ªçc kh√≥a AI t·∫°i AI Vi·ªát Nam (AIO) khai th√°c hi·ªáu qu·∫£ n·ªôi dung t√†i li·ªáu h·ªçc t·∫≠p."
collection: project
author: "Nguy·ªÖn Tu·∫•n Anh - ƒêo√†n T·∫•n H∆∞ng - H·ªì Th·ªã Ng·ªçc Huy·ªÅn - Tr·∫ßn Th·ªã M·ªπ T√∫ - ƒê·∫∑ng Th·ªã Ho√†ng Y·∫øn"
---

T√°c gi·∫£: Nguy·ªÖn Tu·∫•n Anh - ƒêo√†n T·∫•n H∆∞ng - H·ªì Th·ªã Ng·ªçc Huy·ªÅn - Tr·∫ßn Th·ªã M·ªπ T√∫ - ƒê·∫∑ng Th·ªã Ho√†ng Y·∫øn

# T√≥m t·∫Øt
M·∫∑c d√π LLMs r·∫•t m·∫°nh, ch√∫ng v·∫´n b·ªã h·∫°n ch·∫ø v·ªÅ ki·∫øn th·ª©c chuy√™n ng√†nh v√† t√≠nh c·∫≠p nh·∫≠t. D·ª± √°n n√†y x√¢y d·ª±ng h·ªá th·ªëng h·ªèi ƒë√°p th√¥ng minh d√πng ki·∫øn tr√∫c RAG, gi√∫p ng∆∞·ªùi h·ªçc kh√≥a AI t·∫°i AI Vi·ªát Nam (AIO) khai th√°c hi·ªáu qu·∫£ n·ªôi dung t√†i li·ªáu h·ªçc t·∫≠p.


# 1. Gi·ªõi thi·ªáu üóÇ 
- C√°c M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn (LLMs) nh∆∞ ChatGPT c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi linh ho·∫°t nh∆∞ng b·ªã gi·ªõi h·∫°n b·ªüi d·ªØ li·ªáu hu·∫•n luy·ªán, n√™n kh√¥ng x·ª≠ l√Ω t·ªët th√¥ng tin m·ªõi ho·∫∑c c√° nh√¢n h√≥a.
- ƒê·ªÉ kh·∫Øc ph·ª•c, ki·∫øn tr√∫c Retrieval-Augmented Generation (RAG) cho ph√©p LLM truy xu·∫•t th√¥ng tin t·ª´ ngu·ªìn ngo√†i (nh∆∞ PDF, c∆° s·ªü d·ªØ li·ªáu) tr∆∞·ªõc khi t·∫°o c√¢u tr·∫£ l·ªùi, gi√∫p k·∫øt qu·∫£ ch√≠nh x√°c v√† ph√π h·ª£p h∆°n.
- M·ª•c ti√™u d·ª± √°n l√† x√¢y d·ª±ng chatbot ·ª©ng d·ª•ng RAG, h·ªó tr·ª£ h·ªçc vi√™n kh√≥a AIO h·ªèi ‚Äì ƒë√°p tr·ª±c ti·∫øp d·ª±a tr√™n n·ªôi dung t√†i li·ªáu b√†i gi·∫£ng.

# 2. Ph∆∞∆°ng ph√°p lu·∫≠n üìö 
H·ªá th·ªëng ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n ki·∫øn tr√∫c RAG ti√™u chu·∫©n, bao g·ªìm hai quy tr√¨nh ch√≠nh: L·∫≠p ch·ªâ m·ª•c d·ªØ li·ªáu (Indexing) v√† Truy v·∫•n & T·∫°o sinh (Retrieval & Generation).

![Quy tr√¨nh RAG t·ªïng quan](/AIO.github.io/images/M01/M01_RAG_1.png)

H√¨nh 1: S∆° ƒë·ªì t·ªïng quan v·ªÅ ch∆∞∆°ng tr√¨nh RAG trong project.


## 2.1. Quy tr√¨nh L·∫≠p ch·ªâ m·ª•c d·ªØ li·ªáu (Indexing)

<details>
<summary>B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu ‚Äì ƒê·ªçc v√† tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file PDF (PyPDFLoader) </summary>
<pre><code class="language-python">
#H√†m PyPDFLoader
from langchain.document_loaders import PyPDFLoader

# T·∫£i file PDF v√† tr√≠ch xu·∫•t vƒÉn b·∫£n
loader = PyPDFLoader(tmp_file_path)
documents = loader.load()
</code></pre>
</details>

<details>
<summary>B∆∞·ªõc 2: Ph√¢n ƒëo·∫°n ‚Äì Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè (chunks) c√≥ √Ω nghƒ©a (SemanticChunker) </summary>
<pre><code class="language-python">
#H√†m SemanticChunker
from langchain.text_splitter import SemanticChunker

semantic_splitter = SemanticChunker(
    embeddings = st.session_state.embeddings,
    buffer_size = 1,
    breakpoint_threshold_type = "percentile",
    breakpoint_threshold_amount = 95,
    min_chunk_size = 500,
    add_start_index = True
)
</code></pre>
</details>

![Semantic Chunking](/AIO.github.io/images/M01/M01_RAG_3.png)

H√¨nh 2: S∆° ƒë·ªì v·ªÅ Semantic Chunking.

<details>
<summary>B∆∞·ªõc 3: M√£ h√≥a ‚Äì Chuy·ªÉn m·ªói ƒëo·∫°n vƒÉn b·∫£n th√†nh vector s·ªë h·ªçc (bkai-foundation-models/vietnamese-bi-encoder) </summary>

<pre><code class="language-python">
#H√†m bkai-foundation-models/vietnamese-bi-encoder
from langchain.embeddings import HuggingFaceEmbeddings

def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")
</code></pre>
</details>

![Vector database](/AIO.github.io/images/M01/M01_RAG_2.png)

H√¨nh 3: S∆° ƒë·ªì b∆∞·ªõc th·ª±c hi·ªán x√¢y d·ª±ng vector database.


<details>
<summary>B∆∞·ªõc 4: L∆∞u tr·ªØ ‚Äì L∆∞u c√°c vector v√†o c∆° s·ªü d·ªØ li·ªáu ƒë·ªÉ truy v·∫•n nhanh (langchain.vectorstores, Chroma) </summary>
<pre><code class="language-python">    
from langchain.vectorstores import Chroma

#ChromaDB, langchain.vectorstores

# Ph√¢n ƒëo·∫°n v√† l∆∞u tr·ªØ vector
docs = semantic_splitter.split_documents(documents)
vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
retriever = vector_db.as_retriever()

# T·∫£i prompt m·∫´u t·ª´ hub
from langchain import hub
prompt = hub.pull("rlm/rag-prompt")
</code></pre>
</details>


## 2.2. Quy tr√¨nh Truy v·∫•n v√† T·∫°o sinh (Retrieval & Generation)

<details>
<summary>B∆∞·ªõc 1: M√£ h√≥a c√¢u h·ªèi ‚Äì Chuy·ªÉn c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh vector (bkai-foundation-models/vietnamese-bi-encode) </summary>
<pre><code class="language-python">
#H√†m bkai-foundation-models/vietnamese-bi-encoder
@st.cache_resource
def load_embeddings():
  return HuggingFaceEmbeddings(model_name = "bkai-foundation-models/vietnamese-bi-encoder")
</code></pre>
</details>

<details>
<summary>B∆∞·ªõc 2: Truy v·∫•n ‚Äì T√¨m ki·∫øm c√°c ƒëo·∫°n vƒÉn b·∫£n li√™n quan nh·∫•t trong c∆° s·ªü d·ªØ li·ªáu (ChromaDB)</summary>
<pre><code class="language-python">
#H√†m ChromaDB
vector_db = Chroma.from_documents(documents=docs,embedding=st.session_state.embeddings)
</code></pre>
</details>


<details>
<summary>B∆∞·ªõc 3: TƒÉng c∆∞·ªùng ‚Äì K·∫øt h·ª£p c√¢u h·ªèi v√† ƒëo·∫°n vƒÉn b·∫£n th√†nh m·ªôt prompt ho√†n ch·ªânh (rlm/rag-prompt) </summary>    
<pre><code class="language-python">
 rlm/rag-prompt
</code></pre>
</details>


<details>
<summary>B∆∞·ªõc 4: T·∫°o sinh ‚Äì D·ª±a v√†o prompt ƒë√£ tƒÉng c∆∞·ªùng ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi cu·ªëi c√πng (lmsys/vicuna-7b-v1.5) </summary>
<pre><code class="language-python">
#H√†m lmsys/vicuna-7b-v1.5  
def load_llm():
  MODEL_NAME = "lmsys/vicuna-7b-v1.5"
  nf4_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
  )
  model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=nf4_config, low_cpu_mem_usage = True
  )
  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
  model_pipeline = pipeline(
    "text-generation",
    model = model,
    tokenizer = tokenizer,
    max_new_tokens = 512,
    pad_token_id = tokenizer.eos_token_id,
    device_map = "auto"
  )
  return HuggingFacePipeline(pipeline = model_pipeline)
</code></pre>
</details>


# 3. Th·ª±c hi·ªán ‚öô 

·ª®ng d·ª•ng ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng Python v·ªõi giao di·ªán ng∆∞·ªùi d√πng t∆∞∆°ng t√°c ƒë∆∞·ª£c t·∫°o b·ªüi th∆∞ vi·ªán Streamlit. C√°c th∆∞ vi·ªán ch√≠nh ƒë∆∞·ª£c s·ª≠ d·ª•ng bao g·ªìm:
- Streamlit: X√¢y d·ª±ng giao di·ªán web cho ·ª©ng d·ª•ng.
- LangChain: Framework ch√≠nh ƒë·ªÉ k·∫øt n·ªëi c√°c th√†nh ph·∫ßn trong chu·ªói RAG.
- Hugging Face Transformers: T·∫£i v√† v·∫≠n h√†nh c√°c m√¥ h√¨nh embedding v√† LLM.
- ChromaDB: X√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu vector.
- PyPDF: X·ª≠ l√Ω file PDF.

Giao di·ªán ·ª©ng d·ª•ng cho ph√©p ng∆∞·ªùi d√πng:
- T·∫£i l√™n m·ªôt file t√†i li·ªáu PDF.
- Nh·∫•n n√∫t "X·ª≠ l√Ω PDF" ƒë·ªÉ kh·ªüi t·∫°o quy tr√¨nh l·∫≠p ch·ªâ m·ª•c.
- Nh·∫≠p c√¢u h·ªèi v√†o m·ªôt khung chat.
- Nh·∫≠n c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c t·∫°o ra b·ªüi h·ªá th·ªëng.

ƒê·ªÉ t·ªëi ∆∞u h√≥a tr·∫£i nghi·ªám, c√°c m√¥ h√¨nh n·∫∑ng (embedding v√† LLM) ƒë∆∞·ª£c cache l·∫°i b·∫±ng @st.cache_resource c·ªßa Streamlit, ƒë·∫£m b·∫£o ch√∫ng ch·ªâ c·∫ßn t·∫£i m·ªôt l·∫ßn duy nh·∫•t khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng

# 4. K·∫øt qu·∫£ üìà 

File code ho√†n ch·ªânh v√† h√¨nh ·∫£nh giao di·ªán c·ªßa ng∆∞·ªùi d√πng v√† k·∫øt qu·∫£ chatbot b·∫±ng RAG ƒë∆∞·ª£c ghi nh·∫≠n sau ƒë√¢y.

[üëâ Xem file code](/AIO.github.io/files/M01_rag_chatbot.py)


![T·∫£i model](/AIO.github.io/images/M01/M1-1.png)

H√¨nh 4.1: Giao di·ªán c·ªßa ng∆∞·ªùi d√πng - T·∫£i model.


![T·∫£i file](/AIO.github.io/images/M01/M1-2.png)

H√¨nh 4.2: Giao di·ªán c·ªßa ng∆∞·ªùi d√πng - Model ƒë√£ s·∫µn s√†ng v√† t·∫£i file.


![X·ª≠ l√Ω file](/AIO.github.io/images/M01/M1-3.png)

H√¨nh 4.3: Giao di·ªán c·ªßa ng∆∞·ªùi d√πng - X·ª≠ l√Ω file.


![Chatbot tr·∫£ l·ªùi](/AIO.github.io/images/M01/M1-5.png)

H√¨nh 4.4: Giao di·ªán c·ªßa ng∆∞·ªùi d√πng - ƒê·∫∑t c√¢u h·ªèi v√† chatbot tr·∫£ l·ªùi.


# 5. M·ªü r·ªông n√¢ng cao üñ•

ƒêi·ªÉm c·∫£i ti·∫øn sau khi th·ª±c hi·ªán d·ª± √°n ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t nh∆∞ nhau:

## **5.1 Ti√™u ch√≠ c·∫£i ti·∫øn:**

| Ti√™u ch√≠                     | Phi√™n b·∫£n c≈©                                                                                           | Phi√™n b·∫£n c·∫£i ti·∫øn                                                                                                   |
|-----------------------------|----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| **Kh·∫£ nƒÉng ghi nh·ªõ**        | ‚ùå Kh√¥ng ghi nh·ªõ h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥. M·ªói c√¢u h·ªèi ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªôc l·∫≠p.                                                       | ‚úÖ Ghi nh·ªõ v√† s·ª≠ d·ª•ng l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c v√† m·∫°ch l·∫°c trong h·ªôi tho·∫°i.                                             |
| **C√°ch g·ªçi `rag_chain`**    | G·ªçi v·ªõi **ch·ªâ c√¢u h·ªèi**:<br>`rag_chain.invoke(user_input)`                                                                 | G·ªçi v·ªõi **c√¢u h·ªèi + l·ªãch s·ª≠ h·ªôi tho·∫°i**:<br>`rag_chain.invoke({ "question": user_input, "chat_history": retrieve_chat_history() })`   |
| **Thi·∫øt k·∫ø prompt**         | S·ª≠ d·ª•ng prompt m·∫∑c ƒë·ªãnh t·ª´ hub, kh√¥ng c√≥ th√¥ng tin h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥.                                                     | Prompt h·ªó tr·ª£ t√≠ch h·ª£p l·ªãch s·ª≠ h·ªôi tho·∫°i, t√πy ch·ªçn ti·∫øng Vi·ªát/Anh, cho ph√©p th·ª≠ nghi·ªám linh ho·∫°t h∆°n.                                |
| **·ª®ng d·ª•ng th·ª±c t·∫ø**        | Ph√π h·ª£p v·ªõi truy v·∫•n ƒë∆°n l·∫ª, kh√¥ng c·∫ßn b·ªëi c·∫£nh tr∆∞·ªõc ƒë√≥.                                                                  | Th√≠ch h·ª£p cho c√°c cu·ªôc h·ªôi tho·∫°i nhi·ªÅu l∆∞·ª£t c·∫ßn hi·ªÉu ng·ªØ c·∫£nh.                                                                        |
| **C·∫•u tr√∫c & Module h√≥a**   | M√£ ngu·ªìn ƒë∆°n gi·∫£n, √≠t t√°ch module.                                                                                         | C·∫•u tr√∫c r√µ r√†ng, chia module t·ªët v·ªõi c√°c h√†m ri√™ng nh∆∞ `build_prompt_...`, `get_chroma_client()`.                                    |
| **Qu·∫£n l√Ω Vector DB**       | S·ª≠ d·ª•ng ChromaDB theo m·∫∑c ƒë·ªãnh, d·ªÖ l·ªói khi x·ª≠ l√Ω nhi·ªÅu file PDF li√™n ti·∫øp.                                                  | D√πng `chromadb.PersistentClient` v√† `reset()` tr∆∞·ªõc khi x·ª≠ l√Ω file m·ªõi gi√∫p tr√°nh l·ªói v√† qu·∫£n l√Ω tr·∫°ng th√°i t·ªët h∆°n.                 |
| **K·ªπ thu·∫≠t Prompt**         | Duy nh·∫•t m·ªôt prompt t·ª´ `hub.pull("rlm/rag-prompt")`.                                                                      | C√≥ nhi·ªÅu t√πy ch·ªçn prompt: ti·∫øng Vi·ªát, ti·∫øng Anh, t√≠ch h·ª£p l·ªãch s·ª≠ h·ªôi tho·∫°i.                                                         |
| **Giao di·ªán ng∆∞·ªùi d√πng (UI)**| Giao di·ªán ƒë∆°n gi·∫£n.                                                                                                        | C√≥ n√∫t "X√≥a l·ªãch s·ª≠ chat", hi·ªÉn th·ªã logo (`st.logo`), c√°c n√∫t s·ª≠ d·ª•ng `use_container_width=True` gi√∫p UI g·ªçn g√†ng, hi·ªán ƒë·∫°i h∆°n.     |
| **G·ª° l·ªói (Debugging)**      | Kh√¥ng c√≥ logging.                                                                                                           | C√≥ t√≠ch h·ª£p `logger` ƒë·ªÉ ghi l·∫°i th√¥ng tin debug (v√≠ d·ª•: c√°c chunks ƒë∆∞·ª£c truy v·∫•n), h·ªó tr·ª£ ph√°t tri·ªÉn t·ªët h∆°n.                        |
| **Th∆∞ vi·ªán ph·ª• thu·ªôc**      | √çt th∆∞ vi·ªán h∆°n.                                                                                                            | Th√™m th∆∞ vi·ªán nh∆∞ `chromadb`, `ChatPromptTemplate`, `itemgetter` v√† module `utils` t√πy ch·ªânh.                                         |


##  5.2 Code n√¢ng cao

File code c·∫£i ti·∫øn v√† nh·ªØng h√†m s·ª≠ d·ª•ng th√™m trong b√°o c√°o ƒë∆∞·ª£c li·ªát k√™ sau ƒë√¢y.
[üëâ Xem file code c·∫£i ti·∫øn](/AIO.github.io/files/M01_rag_chatbot_cai_tien.py)

### 5.2.1 N√¢ng c·∫•p c·ªët l·ªói: Ghi nh·ªõ l·ªãch s·ª≠ h·ªôi tho·∫°i (Conversation memory) 
<details>
<summary>5.2.1.1. H√†m x√¢y d·ª±ng prompt c√≥ ch·ª©a l·ªãch s·ª≠ h·ªôi tho·∫°i (build_prompt_ragprompt_withhistory_en) </summary>
<pre><code class="language-python">
#H√†m build_prompt_ragprompt_withhistory_en
def build_prompt_ragprompt_withhistory_en():
    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context and conversation history to answer the question. If you don't know the answer, just say that you don't know. 
    Instructions:
    - Use three sentences maximum
    - Keep the answer concise

    Conversation history:
    {chat_history}
    
    Context:
    {context} 

    Question: {question} 

    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt
</code></pre>
</details>


<details>
<summary>5.2.1.2. H√†m ƒë·ªãnh d·∫°ng v√† truy xu·∫•t l·ªãch s·ª≠ chat (retrieve_chat_history, chat_history)</summary>
<pre><code class="language-python">
#H√†m retrieve_chat_history, format_history
def retrieve_chat_history():
    message_threshold = 10
    return st.session_state.chat_history[-message_threshold:] if len(st.session_state.chat_history) >= message_threshold else st.session_state.chat_history

def format_history(histories):
    formatted_history = ""
    for msg in histories:
        role = "User" if msg["role"] == "user" else "Assistant"
        formatted_history += f"{role}: {msg['content']}\n\n"
    return formatted_history.strip()
</code></pre>
</details>

<details>
<summary>5.2.1.3. C·∫≠p nh·∫≠t RAG Chain ƒë·ªÉ x·ª≠ l√Ω l·ªãch s·ª≠ chat (process_pdf_updated_chain(retriever, llm)) </summary>
<pre><code class="language-python">
#H√†m process_pdf_updated_chain(retriever, llm)
def process_pdf_updated_chain(retriever, llm):
    prompt = build_prompt_ragprompt_withhistory_en()
    rag_chain = (
        {
            "context": itemgetter("question") | retriever | format_docs,
            "question": itemgetter("question"),
            "chat_history": lambda x: format_history(x["chat_history"])
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain
</code></pre>
</details>


<details>
<summary>5.2.1.4. C·∫≠p nh·∫≠t c√°ch g·ªçi RAG chain (main_updated_invoke) </summary>
<pre><code class="language-python">
#H√†m main_updated_invoke
def main_updated_invoke(user_input):
    output = st.session_state.rag_chain.invoke({
        "question": user_input,
        "chat_history": retrieve_chat_history()
    })
</code></pre>
</details>

### 5.2.2 Qu·∫£n l√Ω Vector DB n√¢ng cao
<details>
<summary>X√¢y d·ª±ng h√†m get_chroma_client, process_pdf_updated_db_handling </summary>
<pre><code class="language-python">
def get_chroma_client(allow_reset=False):
    """Get a Chroma client for vector database operations."""
    return chromadb.PersistentClient(settings=chromadb.Settings(allow_reset=allow_reset))

def process_pdf_updated_db_handling():
    client = get_chroma_client(allow_reset=True)
    client.reset()
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=st.session_state.embeddings,
        client=client
    )
</code></pre>
</details>


### 5.2.3. G·ª° l·ªói (Debugging) v·ªõi Logger
<details>
<summary>X√¢y d·ª±ng h√†m format_docs_with_logging </summary>

<pre><code class="language-python">
def format_docs_with_logging(docs):
    logger.info(f"**Debug: Retrieved {len(docs)} chunks:**")
    for i, doc in enumerate(docs):
        page_num = doc.metadata.get('page') + 1 if 'page' in doc.metadata else -1
        source = doc.metadata.get('source', 'document')
        file_name = os.path.basename(source) if isinstance(source, str) else 'unknown'

        logger.info(f"""
        ([reference-{i+1}] Page {page_num} - Source: {file_name})
        {doc.page_content}""")
    
    return "\n\n".join(doc.page_content for doc in docs)
</code></pre>
</details>

### 5.2.4. C·∫£i ti·∫øn giao di·ªán ng∆∞·ªùi d√πng (UI)
<details>
<summary>X√¢y d·ª±ng h√†m  main_sidebar_enhancements </summary>
<pre><code class="language-python">
def main_sidebar_enhancements():
    with st.sidebar:
        st.logo("./assets/logo.png")
        st.subheader("üí¨ ƒêi·ªÅu khi·ªÉn Chat")
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            clear_chat()
            st.rerun()
</code></pre>
</details>

##  5.3 K·∫øt qu·∫£ üìç

H√¨nh ·∫£nh giao di·ªán c·ªßa ng∆∞·ªùi d√πng v√† k·∫øt qu·∫£ chatbot b·∫±ng RAG sau khi c·∫£i ti·∫øn ƒë∆∞·ª£c ghi nh·∫≠n sau ƒë√¢y.

![Data m·∫´u YOLOv10_Tutorials](/AIO.github.io/images/M01/M1-6.png)

H√¨nh 5: K·∫øt qu·∫£ giao di·ªán c·ªßa ng∆∞·ªùi d√πng v·ªõi file Data m·∫´u YOLOv10_Tutorials.pdf


![file Medical Report](/AIO.github.io/images/M01/M1-7.png)

H√¨nh 6: K·∫øt qu·∫£ giao di·ªán c·ªßa ng∆∞·ªùi d√πng v·ªõi file Medical Report

# 6. K·∫øt lu·∫≠n üìå 

- D·ª± √°n ƒë√£ **x√¢y d·ª±ng th√†nh c√¥ng m·ªôt chatbot ·ª©ng d·ª•ng ki·∫øn tr√∫c RAG, c√≥ kh·∫£ nƒÉng h·ªèi ƒë√°p tr·ª±c ti·∫øp v√† hi·ªáu qu·∫£ v·ªõi c√°c t√†i li·ªáu PDF chuy√™n bi·ªát**, ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh b·∫±ng c√°ch k·∫øt h·ª£p truy v·∫•n th√¥ng tin c·ªßa c∆° s·ªü d·ªØ li·ªáu vector v√† kh·∫£ nƒÉng t·∫°o sinh ng√¥n ng·ªØ c·ªßa LLMs.
  
- Ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi c·ªßa h·ªá th·ªëng **ph·ª• thu·ªôc ho√†n to√†n v√†o hi·ªáu qu·∫£ c·ªßa b∆∞·ªõc truy v·∫•n th√¥ng tin (retrieval)**. N·∫øu qu√° tr√¨nh t√¨m ki·∫øm ng·ªØ nghƒ©a kh√¥ng t√¨m ƒë∆∞·ª£c ƒë√∫ng ƒëo·∫°n vƒÉn b·∫£n ch·ª©a th√¥ng tin li√™n quan trong Vector Database, m√¥ h√¨nh LLM s·∫Ω kh√¥ng c√≥ ƒë·ªß ng·ªØ c·∫£nh c·∫ßn thi·∫øt, d·∫´n ƒë·∫øn nguy c∆° t·∫°o ra c√¢u tr·∫£ l·ªùi sai, kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
  
- V·ªõi ph∆∞∆°ng ph√°p n√†y, d·ª± √°n m·ªü ra nhi·ªÅu h∆∞·ªõng ph√°t tri·ªÉn ti·ªÅm nƒÉng trong t∆∞∆°ng lai ƒë·ªÉ **ti·∫øp t·ª•c t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô, ƒë·ªô ch√≠nh x√°c v√† n√¢ng cao tr·∫£i nghi·ªám ng∆∞·ªùi d√πng**.
