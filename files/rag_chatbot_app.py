import streamlit as st
import tempfile
import os
import torch
import chromadb
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_huggingface.llms import HuggingFacePipeline
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
import time
from operator import itemgetter
from utils.logging_utils import logger

# Session state initialization
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_processed' not in st.session_state:
    st.session_state.pdf_processed = False
if 'pdf_name' not in st.session_state:
    st.session_state.pdf_name = ""

# Functions
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")

def get_chroma_client(allow_reset=False):
    """Get a Chroma client for vector database operations."""
    # Use PersistentClient for persistent storage
    return chromadb.PersistentClient(settings=chromadb.Settings(allow_reset=allow_reset))

@st.cache_resource  
def load_llm():
    MODEL_NAME = "lmsys/vicuna-7b-v1.5"

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4"
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto"
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    model_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        pad_token_id=tokenizer.eos_token_id,
        device_map="auto"
    )
    
    return HuggingFacePipeline(pipeline=model_pipeline)


def build_prompt_fromhub_ragprompt():
    """Build a prompt for the RAG chain."""
    # Load the prompt from the hub: "rlm/rag-prompt"
    return hub.pull("rlm/rag-prompt")

def build_prompt_ragprompt_en():
    # This is the exact prompt from "rlm/rag-prompt" hub
    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
    Question: {question} 
    Context: {context} 
    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def build_prompt_ragprompt_vn():
    # This is the exact prompt from "rlm/rag-prompt" hub
    template = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ chuyÃªn thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ há»i-Ä‘Ã¡p. HÃ£y sá»­ dá»¥ng nhá»¯ng Ä‘oáº¡n ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t sau Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. Náº¿u báº¡n khÃ´ng biáº¿t cÃ¢u tráº£ lá»i, chá»‰ cáº§n nÃ³i ráº±ng báº¡n khÃ´ng biáº¿t. Tráº£ lá»i tá»‘i Ä‘a ba cÃ¢u vÃ  giá»¯ cho cÃ¢u tráº£ lá»i ngáº¯n gá»n.
    Question: {question} 
    Context: {context} 
    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def build_prompt_ragprompt_withhistory_en():
    # This is the exact prompt from "rlm/rag-prompt" hub


    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context and conversation history to answer the question. If you don't know the answer, just say that you don't know. 
    Instructions:
    - Use three sentences maximum
    - Keep the answer concise

    Conversation history:
    {chat_history}
    
    Context:
    {context} 

    Question: {question} 

    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def build_prompt_v2():
    # Build a custom prompt for the RAG chain
    template = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, chuyÃªn tráº£ lá»i cÃ¡c cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.
    Context:
    {context}
    
    HÃ£y tráº£ lá»i cÃ¢u há»i sau Ä‘Ã¢y má»™t cÃ¡ch ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c:
    {question}
    
    Tráº£ lá»i cá»§a báº¡n nÃªn dá»±a trÃªn thÃ´ng tin trong tÃ i liá»‡u vÃ  khÃ´ng Ä‘Æ°á»£c thÃªm báº¥t ká»³ thÃ´ng tin nÃ o khÃ´ng cÃ³ trong Ä‘Ã³.
    """
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def retrieve_chat_history():
    # Retrieve the last x messages from chat history
    message_threshold = 10  # Number of messages to retrieve
    return st.session_state.chat_history[-message_threshold:] if len(st.session_state.chat_history) >= message_threshold else st.session_state.chat_history

#@st.cache_resource
def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, prefix = uploaded_file.name, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    
    semantic_splitter = SemanticChunker(
        embeddings=st.session_state.embeddings,
        buffer_size=1,
        breakpoint_threshold_type="percentile", 
        breakpoint_threshold_amount=95,
        min_chunk_size=500,
        add_start_index=True
    )
    
    docs = semantic_splitter.split_documents(documents)
    # Fix: Use ephemeral ChromaDB client to avoid tenant error
    # client = chromadb.EphemeralClient()
    client = get_chroma_client(allow_reset=True)
    client.reset()  # Reset client to ensure no previous state
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=st.session_state.embeddings,
        client=client
    )
    retriever = vector_db.as_retriever()
    
    #prompt = hub.pull("rlm/rag-prompt")
    prompt = build_prompt_ragprompt_withhistory_en()

    def format_docs(docs):
        logger.info(f"**Debug: Retrieved {len(docs)} chunks:**")
        for i, doc in enumerate(docs):
            # Extract metadata if available
            # Assuming each doc has metadata with 'page' and 'source'
            page_num = doc.metadata.get('page') + 1 if 'page' in doc.metadata else -1
            source = doc.metadata.get('source', 'document')
            file_name = os.path.basename(source) if isinstance(source, str) else 'unknown'

            logger.info(f"""
            ([reference-{i+1}] Page {page_num} - Source: {file_name})
            {doc.page_content}""")
        
        return "\n\n".join(doc.page_content for doc in docs)
    
    def format_history(histories):
        formatted_history = ""
        for msg in histories:
            role = "User" if msg["role"] == "user" else "Assistant"
            formatted_history += f"{role}: {msg['content']}\n\n"
        return formatted_history.strip()
    
    
    rag_chain = (
        {
            "context": itemgetter("question") | retriever | format_docs,
            #"question": RunnablePassthrough(),
            "question": itemgetter("question"),
            "chat_history": lambda x: format_history(x["chat_history"])
        }
        | prompt
        | st.session_state.llm
        | StrOutputParser()
    )
    
    os.unlink(tmp_file_path)
    return rag_chain, len(docs)

def add_message(role, content):
    """ThÃªm tin nháº¯n vÃ o lá»‹ch sá»­ chat"""
    st.session_state.chat_history.append({
        "role": role,
        "content": content,
        "timestamp": time.time()
    })

def clear_chat():
    """XÃ³a lá»‹ch sá»­ chat"""
    st.session_state.chat_history = []

def display_chat():
    """Hiá»ƒn thá»‹ lá»‹ch sá»­ chat"""
    if st.session_state.chat_history:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write("Xin chÃ o! TÃ´i lÃ  AI assistant. HÃ£y upload file PDF vÃ  báº¯t Ä‘áº§u Ä‘áº·t cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u nhÃ©! ğŸ˜Š")

# UI
def main():
    st.set_page_config(
        page_title="PDF RAG Chatbot", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    st.title("PDF RAG Assistant")

    # Trong streamlit v-1.38 khÃ´ng khá»— trá»£ param size
    st.logo("./assets/logo.png", size="large")
    
    # Sidebar
    with st.sidebar:
        st.title("âš™ï¸ CÃ i Ä‘áº·t")
        
        # Load models
        if not st.session_state.models_loaded:
            st.warning("â³ Äang táº£i models...")
            with st.spinner("Äang táº£i AI models..."):
                st.session_state.embeddings = load_embeddings()
                st.session_state.llm = load_llm()
                st.session_state.models_loaded = True
            st.success("âœ… Models Ä‘Ã£ sáºµn sÃ ng!")
            st.rerun()
        else:
            st.success("âœ… Models Ä‘Ã£ sáºµn sÃ ng!")

        st.markdown("---")
        
        # Upload PDF
        st.subheader("ğŸ“„ Upload tÃ i liá»‡u")
        uploaded_file = st.file_uploader("Chá»n file PDF", type="pdf")
        
        if uploaded_file:
            if st.button("ğŸ”„ Xá»­ lÃ½ PDF", use_container_width=True):
                with st.spinner("Äang xá»­ lÃ½ PDF..."):
                    st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)
                    st.session_state.pdf_processed = True
                    st.session_state.pdf_name = uploaded_file.name
                    # Reset chat history khi upload PDF má»›i
                    clear_chat()
                    add_message("assistant", f"âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng file **{uploaded_file.name}**!\n\nğŸ“Š TÃ i liá»‡u Ä‘Æ°á»£c chia thÃ nh {num_chunks} pháº§n. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u Ä‘áº·t cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u.")
                st.rerun()
        
        # PDF status
        if st.session_state.pdf_processed:
            st.success(f"ğŸ“„ ÄÃ£ táº£i: {st.session_state.pdf_name}")
        else:
            st.info("ğŸ“„ ChÆ°a cÃ³ tÃ i liá»‡u")
            
        st.markdown("---")
        
        # Chat controls
        st.subheader("ğŸ’¬ Äiá»u khiá»ƒn Chat")
        if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat", use_container_width=True):
            clear_chat()
            st.rerun()
            
        st.markdown("---")
        
        # Instructions
        st.subheader("ğŸ“‹ HÆ°á»›ng dáº«n")
        st.markdown("""
        **CÃ¡ch sá»­ dá»¥ng:**
        1. **Upload PDF** - Chá»n file vÃ  nháº¥n "Xá»­ lÃ½ PDF"
        2. **Äáº·t cÃ¢u há»i** - Nháº­p cÃ¢u há»i trong Ã´ chat
        3. **Nháº­n tráº£ lá»i** - AI sáº½ tráº£ lá»i dá»±a trÃªn ná»™i dung PDF
        """)

    # Main content
    st.markdown("*TrÃ² chuyá»‡n vá»›i Chatbot Ä‘á»ƒ trao Ä‘á»•i vá» ná»™i dung tÃ i liá»‡u PDF cá»§a báº¡n*")
    
    # Chat container
    chat_container = st.container()
    
    with chat_container:
        # Display chat history
        display_chat()
    
    # Chat input
    if st.session_state.models_loaded:
        if st.session_state.pdf_processed:
            # User input
            user_input = st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n...")
            
            if user_input:
                # Add user message
                add_message("user", user_input)
                
                # Display user message immediately
                with st.chat_message("user"):
                    st.write(user_input)
                
                # Generate response
                with st.chat_message("assistant"):
                    with st.spinner("Äang suy nghÄ©..."):
                        try:
                            # output = st.session_state.rag_chain.invoke(user_input)
                            output = st.session_state.rag_chain.invoke({
                                "question": user_input,
                                "chat_history": retrieve_chat_history()
                            })
                            # Clean up the response
                            if 'Answer:' in output:
                                answer = output.split('Answer:')[1].strip()
                            else:
                                answer = output.strip()
                            
                            # Display response
                            st.write(answer)
                            
                            # Add assistant message to history
                            add_message("assistant", answer)
                            
                        except Exception as e:
                            logger.error(e, exc_info=True)
                            error_msg = f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}"
                            st.error(error_msg)
                            add_message("assistant", error_msg)
        else:
            st.info("ğŸ”„ Vui lÃ²ng upload vÃ  xá»­ lÃ½ file PDF trÆ°á»›c khi báº¯t Ä‘áº§u chat!")
            st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n...", disabled=True)
    else:
        st.info("â³ Äang táº£i AI models, vui lÃ²ng Ä‘á»£i...")
        st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n...", disabled=True)

if __name__ == "__main__":
    main()
